# Implementation Plan: Conversational AI Agent Interface

**Branch**: `003-ai-agent-interface` | **Date**: 2026-02-17 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/003-ai-agent-interface/spec.md`

---

## Summary

Add a conversational AI agent to the existing Todo web app. Users type natural-
language messages (e.g. "add a task called X", "delete it") into a chat panel
embedded in the dashboard. The backend verifies the session token, fetches the
user's full conversation history, initialises a fresh OpenAI Agent with five
MCP tools (add / list / update / complete / delete task), runs the agent loop,
saves the response, and returns it. The agent never touches the database
directly — all data access goes through the MCP tool layer. The server is
completely stateless between requests.

---

## Technical Context

**Language/Version**: Python 3.12+ (backend), TypeScript/Node 20+ (frontend)
**Primary Dependencies**:
- Backend: FastAPI, SQLModel, asyncpg, `openai-agents`, `mcp`
- Frontend: Next.js 16+, Tailwind CSS, Lucide React (existing)
**Storage**: Neon Serverless PostgreSQL — 2 new tables (`conversation`, `message`)
**Testing**: Manual acceptance testing per quickstart.md
**Target Platform**: Linux server (Docker), browser (Chrome/Safari/Firefox)
**Project Type**: Hybrid web + AI agent (monorepo)
**Performance Goals**: Agent response under 5 seconds for single-tool operations
**Constraints**: Stateless server — no in-memory state between requests
**Scale/Scope**: Single user thread per user, MVP; ~50-turn context window assumed

---

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Principle | Check | Result |
|-----------|-------|--------|
| I — Monorepo | Agent in `backend/app/agent/`, MCP server in `backend/app/mcp_server/` | ✅ PASS |
| II — Tech Stack | FastAPI ✅, OpenAI Agents SDK ✅, Official MCP SDK ✅, SQLModel ✅, Neon ✅, Better Auth ✅ | ✅ PASS |
| III — Security | `user_id` extracted from verified session token at HTTP layer; passed to every MCP tool; never from message body | ✅ PASS |
| IV — Dev Process | spec.md ✅, plan.md ✅ (this file), contracts/ ✅, tasks.md (next step) | ✅ PASS |
| V — AI Agent & MCP | Stateless 5-step lifecycle; all 5 tools via MCP SDK; agent initialised fresh per request; typed args + dict returns | ✅ PASS |

**Gate result**: All principles pass. Proceed to implementation.

---

## Project Structure

### Documentation (this feature)

```text
specs/003-ai-agent-interface/
├── plan.md              ← this file
├── spec.md
├── research.md
├── data-model.md
├── quickstart.md
├── contracts/
│   ├── rest-api.md
│   └── mcp-tools.md
├── checklists/
│   └── requirements.md
└── tasks.md             ← generated by /sp.tasks
```

### Source Code

```text
backend/
└── app/
    ├── models/
    │   ├── task.py             (unchanged)
    │   ├── conversation.py     ← NEW
    │   └── message.py          ← NEW
    ├── mcp_server/
    │   ├── __init__.py         ← NEW
    │   └── server.py           ← NEW  (FastMCP, 5 tools)
    ├── agent/
    │   ├── __init__.py         ← NEW
    │   ├── runner.py           ← NEW  (MCPServerStdio + Agent + Runner)
    │   └── prompts.py          ← NEW  (SYSTEM_PROMPT constant)
    ├── routers/
    │   ├── tasks.py            (unchanged)
    │   └── chat.py             ← NEW  (POST /api/chat, GET /api/chat/history)
    ├── database.py             (unchanged — init_db picks up new models)
    ├── main.py                 ← MODIFIED (register chat router, lifespan for MCP server)
    └── config.py               ← MODIFIED (add OPENAI_API_KEY)

frontend/
└── components/
    ├── dashboard/              (existing components unchanged)
    └── chat/
        ├── ChatPanel.tsx       ← NEW  (message list + input bar)
        ├── ChatMessage.tsx     ← NEW  (single message bubble)
        └── ChatInput.tsx       ← NEW  (textarea + send button)
└── app/
    └── dashboard/
        └── page.tsx            ← MODIFIED (add ChatPanel alongside task grid)
└── lib/
    └── api.ts                  (unchanged — chat calls go through it)
```

---

## Architecture: Request Flow

```
Browser (ChatPanel)
  │  POST /api/chat  { message: "add task X" }
  │  Authorization: Bearer <token>
  ▼
FastAPI (chat.py router)
  │  1. Verify token → extract user_id
  │  2. Fetch or create Conversation for user_id
  │  3. Load all Messages for conversation (ordered by created_at)
  │  4. Call agent_runner.run_agent(user_id, history, message)
  │     │
  │     ▼  agent/runner.py
  │     │  5. Build input list: history + new user message
  │     │  6. Init Agent(instructions=SYSTEM_PROMPT, mcp_servers=[mcp_server])
  │     │  7. await Runner.run(agent, input=messages)
  │     │     │
  │     │     ▼  OpenAI API (gpt-4o)
  │     │     │  Agent decides to call e.g. add_task(...)
  │     │     │
  │     │     ▼  MCPServerStdio → subprocess
  │     │        mcp_server/server.py
  │     │        add_task(user_id, title, description) → dict
  │     │        (opens own DB session, inserts row, returns result)
  │     │
  │     │  8. result.final_output  ← natural language response
  │     ▼
  │  9. Save user Message to DB (role="user")
  │  10. Save assistant Message to DB (role="assistant")
  │  11. Discard Agent instance
  ▼
Response: { response: "...", conversation_id: "..." }
  ▼
Browser — ChatPanel appends new bubble
           Task grid refreshes (refetch tasks)
```

---

## Step-by-Step Implementation

### Step 1 — Database Upgrade

**Files**: `backend/app/models/conversation.py`, `backend/app/models/message.py`

Add two SQLModel table classes. Both are discovered by `init_db()` automatically
through `SQLModel.metadata`.

**`Conversation` model**:
```python
class Conversation(SQLModel, table=True):
    __tablename__ = "conversation"
    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)
    user_id: str = Field(nullable=False, index=True)
    created_at: datetime = Field(default_factory=datetime.utcnow)
```

**`Message` model**:
```python
class Message(SQLModel, table=True):
    __tablename__ = "message"
    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)
    conversation_id: uuid.UUID = Field(foreign_key="conversation.id", nullable=False)
    role: str = Field(nullable=False)          # user | assistant | tool
    content: str = Field(nullable=False)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    # composite index for ordered history fetch:
    __table_args__ = (Index("ix_message_conv_created", "conversation_id", "created_at"),)
```

**Validation gate**: After applying, `init_db()` creates both tables on startup.
Verify with `SELECT * FROM conversation LIMIT 1;` — no error means tables exist.

---

### Step 2 — MCP Server

**File**: `backend/app/mcp_server/server.py`

```python
from mcp.server.fastmcp import FastMCP
import asyncpg  # or use SQLModel session

mcp = FastMCP("task-manager")

@mcp.tool()
async def add_task(user_id: str, title: str, description: str,
                   status: str = "pending") -> dict:
    """Create a new task for the user."""
    # open own DB session, insert Task row, return dict
    ...

@mcp.tool()
async def list_tasks(user_id: str, status: str | None = None) -> dict:
    """List all tasks for the user, optionally filtered by status."""
    ...

@mcp.tool()
async def update_task(user_id: str, task_id: str, title: str,
                      description: str | None = None) -> dict:
    """Update a task's title and optionally description."""
    ...

@mcp.tool()
async def delete_task(user_id: str, task_id: str) -> dict:
    """Delete a task owned by the user."""
    ...

@mcp.tool()
async def complete_task(user_id: str, task_id: str) -> dict:
    """Mark a task as completed."""
    ...

if __name__ == "__main__":
    mcp.run()    # starts stdio transport — used by MCPServerStdio
```

**Key constraint**: Each tool creates its own DB connection using `app.database`
engine — it does NOT share the FastAPI request's `AsyncSession` because it runs
in a subprocess.

**Validation gate**: Run `python -m app.mcp_server.server` manually from
`backend/`. It should start without errors and wait on stdin.

---

### Step 3 — Agent Runner

**Files**: `backend/app/agent/runner.py`, `backend/app/agent/prompts.py`

**`prompts.py`**:
```python
SYSTEM_PROMPT = """
You are TaskFlow Assistant, a helpful task manager for {user_name}.

You help users manage their todo tasks through natural conversation.
You have access to these tools: add_task, list_tasks, update_task,
delete_task, complete_task.

Rules:
- Always use tools to perform task operations — never describe actions
  without executing them.
- Use conversation history to resolve references like "it", "that task",
  "the second one".
- If a reference is ambiguous, ask the user to clarify before acting.
- Respond concisely and confirm what you did.
- Never reveal user_id or internal IDs in your response.
"""
```

**`runner.py`**:
```python
from agents import Agent, Runner, MCPServerStdio

_mcp_server: MCPServerStdio | None = None

async def get_mcp_server() -> MCPServerStdio:
    """Return singleton MCP server subprocess, starting it if needed."""
    global _mcp_server
    if _mcp_server is None:
        _mcp_server = MCPServerStdio(
            params={"command": "python",
                    "args": ["-m", "app.mcp_server.server"]},
            cache_tools_list=True,
        )
        await _mcp_server.__aenter__()
    return _mcp_server

async def run_agent(user_id: str, history: list[dict], new_message: str) -> str:
    """
    Create a fresh Agent per request, run it with full history, return response.
    Stateless — no state persists after this function returns.
    """
    server = await get_mcp_server()
    agent = Agent(
        name="TaskFlowAssistant",
        model="gpt-4o",
        instructions=SYSTEM_PROMPT,
        mcp_servers=[server],
    )
    input_messages = history + [{"role": "user", "content": new_message}]
    result = await Runner.run(agent, input=input_messages)
    return result.final_output

async def shutdown_mcp_server() -> None:
    """Call from FastAPI lifespan on shutdown."""
    global _mcp_server
    if _mcp_server is not None:
        await _mcp_server.__aexit__(None, None, None)
        _mcp_server = None
```

**Validation gate**: Unit-testable by mocking `Runner.run`. Integration test:
call `run_agent("user1", [], "list my tasks")` — should return a string.

---

### Step 4 — Chat Endpoint

**File**: `backend/app/routers/chat.py`

```python
router = APIRouter(prefix="/api/chat", tags=["chat"])

@router.post("/", response_model=ChatResponse)
async def post_chat(
    body: ChatRequest,
    user_id: str = Depends(get_current_user_id),
    db: AsyncSession = Depends(get_session),
):
    # 1. Fetch or create conversation
    conversation = await get_or_create_conversation(db, user_id)
    # 2. Load history (user + assistant roles only for display;
    #    full roles for agent input)
    messages = await load_messages(db, conversation.id)
    history = [{"role": m.role, "content": m.content} for m in messages]
    # 3. Run agent
    response_text = await run_agent(user_id, history, body.message)
    # 4. Save user message
    await save_message(db, conversation.id, "user", body.message)
    # 5. Save assistant response
    await save_message(db, conversation.id, "assistant", response_text)
    # 6. Return response (agent state discarded implicitly)
    return ChatResponse(response=response_text,
                        conversation_id=str(conversation.id))

@router.get("/history", response_model=HistoryResponse)
async def get_history(
    user_id: str = Depends(get_current_user_id),
    db: AsyncSession = Depends(get_session),
):
    conversation = await get_or_create_conversation(db, user_id)
    messages = await load_messages(db, conversation.id)
    # Return only user + assistant (not tool messages)
    visible = [m for m in messages if m.role in ("user", "assistant")]
    return HistoryResponse(
        conversation_id=str(conversation.id),
        messages=[MessageRead.from_orm(m) for m in visible],
    )
```

**Pydantic schemas** (in `chat.py` or `app/schemas/chat.py`):
```python
class ChatRequest(BaseModel):
    message: str = Field(min_length=1, max_length=2000)

class ChatResponse(BaseModel):
    response: str
    conversation_id: str

class MessageRead(BaseModel):
    id: str
    role: str
    content: str
    created_at: datetime

class HistoryResponse(BaseModel):
    conversation_id: str
    messages: list[MessageRead]
```

**Validation gate**: `curl -X POST /api/chat -H "Authorization: Bearer <token>"
-d '{"message": "hello"}'` returns 200 with a `response` string.

---

### Step 5 — Frontend Chat UI

**Files**: `frontend/components/chat/ChatPanel.tsx`,
`frontend/components/chat/ChatMessage.tsx`,
`frontend/components/chat/ChatInput.tsx`

**Integration into dashboard** (`app/dashboard/page.tsx`):
```tsx
// Add ChatPanel to the right side of the content area
<div className="flex gap-6 flex-col xl:flex-row">
  {/* Existing task content — left / full width on mobile */}
  <div className="flex-1 min-w-0"> ... existing task sections ... </div>
  {/* Chat panel — right side on xl screens, bottom on mobile */}
  <div className="w-full xl:w-96 shrink-0">
    <ChatPanel onTaskMutated={fetchTasks} />
  </div>
</div>
```

**`ChatPanel.tsx`** responsibilities:
1. On mount: `GET /api/chat/history` → populate message list.
2. Submit: `POST /api/chat { message }` via `api.post()`.
3. Show typing indicator (spinner) while waiting.
4. Append response bubble.
5. Call `onTaskMutated()` after every successful response to refresh task grid.
6. Disable input while request is in-flight.

**Message bubble styling**:
- User messages: right-aligned, indigo background.
- Assistant messages: left-aligned, gray background, TaskFlow icon.

**Validation gate**: After creating a task via chat, the task card grid
refreshes without a manual reload. The chat panel shows both message bubbles
in the correct order.

---

## System Prompt Design

```
You are TaskFlow Assistant, a friendly task management AI.

Your capabilities (use the appropriate tool for each):
- add_task: when user wants to create a new task
- list_tasks: when user wants to see their tasks
- update_task: when user wants to rename or edit a task
- complete_task: when user wants to mark a task as done
- delete_task: when user wants to remove a task

Context rules:
- Use conversation history to understand references like "it", "that one",
  "the first task", "the one I just added".
- If a reference is ambiguous (e.g. multiple tasks match), ask which one.
- Always confirm what you did in plain language.
- If the request is outside task management, politely decline and suggest
  what you can help with.
- Keep responses brief — 1–3 sentences unless listing tasks.
```

---

## Non-Functional Requirements

| Concern | Decision |
|---------|---------|
| API Key | `OPENAI_API_KEY` in `backend/.env`, loaded via `app/config.py` |
| Model | `gpt-4o` (configurable via `OPENAI_MODEL` env var) |
| MCP process | Singleton subprocess, started on first request, shut down on app exit |
| History volume | All messages loaded for MVP; truncate to last 100 if >100 rows |
| Error handling | Any agent exception returns `500 {"detail": "Agent error"}` |
| CORS | Existing CORS config covers `/api/chat` (same origin rules) |

---

## Complexity Tracking

No constitution violations. The MCP subprocess pattern is the documented
standard for OpenAI Agents SDK + MCP SDK integration and does not add
unjustified complexity.

---

## Follow-ups and Risks

1. **OpenAI API cost**: gpt-4o charges per token. Long conversation histories
   accumulate tokens quickly. Implement a 100-message rolling window cap before
   going to production.
2. **Subprocess stability**: If the MCP server subprocess crashes, the singleton
   becomes stale. Add a health-check + restart guard in `get_mcp_server()`.
3. **Tool ambiguity**: The agent may call the wrong tool for edge-case phrasing.
   Refine the system prompt after observing real usage patterns.
